<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Preety Manna</title>

    <meta name="author" content="Anikait Singh">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Preety Manna
                </p>
                <p>
                  I am a graduate in Electrical and Electronics Engineering by achieveing First Class with Distinction from SRM Chennai.
                </p>
                <p>
                  Previously, I was at UC Berkeley advised by <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>,
                  <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>, and <a href="https://aviralkumar2907.github.io/">Aviral Kumar</a> 
                  as part of BAIR working on Deep RL and Robot Learning.
                </p>
                <p style="text-align:center">
                  <a href="mailto:anikait@stanford.edu">Email</a> &nbsp;/&nbsp;
                  <a href="./static/pdf/resume.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=lPaISmIAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/Anikait_Singh_">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/asap7772/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="./static/images/profile.png"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="./static/images/profile.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <hr>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My primary research interests are in decision-making methods such as reinforcement learning and scaling them up. 
                  I believe that a good target of my research would be to produce foundation models for decision-making that utilize 
                  large diverse data sources that show good generalization and enable rapid learning.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./static/images/understanding_rlhf.jpeg" alt="understanding_rlhf" width="240" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2404.14367.pdf" id="understanding_rlhf_arxiv">
                  <span class="papertitle">Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy Data</span>
                </a>
                <br>
                <strong>Anikait Singh*</strong>, Fahim Tajwar*, Archit Sharma, Rafael Rafailov, 
                Jeff Schneider, Tengyang Xie, Stefano Ermon, Chelsea Finn, Aviral Kumar
                <br>
                <em>ICML</em>, 2024
                <br>
                <a href="https://understanding-rlhf.github.io/">project page</a> /
                <a href="https://arxiv.org/pdf/2404.14367.pdf">paper</a> /
                <a href="https://github.com/Asap7772/understanding-rlhf">code</a>
                <p></p>
                <p>Learning from preferences is a common paradigm for fine-tuning language models. 
                  Yet, many algorithmic design decisions come into play. Our new work finds that 
                  approaches employing on-policy sampling or negative gradients outperform offline, 
                  maximum likelihood objectives.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./static/images/d5rldomains.jpg" alt="understanding_rlhf" width="240" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://openreview.net/pdf?id=Aj1wftldeR" id="understanding_rlhf_arxiv">
                  <span class="papertitle">D5RL: Diverse Datasets for Data-Driven Deep Reinforcement Learning</span>
                </a>
                <br>
                Rafael Rafailov*, Kyle Beltran Hatch*, <strong>Anikait Singh</strong>, Aviral Kumar, 
                Laura Smith, Ilya Kostrikov, Philippe Hansen-Estruch, Victor Kolev, 
                Philip J. Ball, Jiajun Wu, Sergey Levine, Chelsea Finn
                <br>
                <em>RCL</em>, 2024
                <br>
                <a href="https://sites.google.com/view/d5rl/">project page</a> /
                <a href="https://openreview.net/pdf?id=Aj1wftldeR">paper</a> /
                <a href="https://github.com/d5rlbenchmark/d5rl">code</a>
                <p></p>
                <p>Offline RL algorithms enable data-driven methods without the need for 
                  costly or dangerous real-world exploration, leveraging large pre-collected datasets. 
                  However, effective and challenging benchmarks that capture real-world task properties are 
                  necessary for evaluating progress, prompting the proposal of a new benchmark for offline RL 
                  based on realistic robotic simulations and diverse data sources to 
                  support both offline RL and online fine-tuning evaluation.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./static/images/VPTR.png" alt="workflow" width="240" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2309.13041.pdf" id="workflow_arxiv">
                  <span class="papertitle"> Robotic Offline RL from Internet Videos via Value-Function Pre-Training</span>
                </a>
                <br>
                Chethan Bhateja*, Derek Guo*, Dibya Ghosh*, <strong>Anikait Singh</strong>, Manan Tomar,
                <br>
                Quan Vuong, Yevgen Chebotar, Sergey Levine, Aviral Kumar
                <br>
                <em>ICRA</em>, 2024
                <br>
                <a href="https://arxiv.org/pdf/2309.13041.pdf">paper</a> /
                <a href="https://dibyaghosh.com/vptr/">project page</a> /
                <a href="https://dibyaghosh.com/vptr/results.html">videos</a>
                <p></p>
                <p>
                  VPTR is a framework that combines the benefits of pre-training on video data with robotic offline RL approaches 
                  that train on diverse robot data, resulting in value functions and policies for manipulation tasks that are robust 
                  and generalizable.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./static/images/RTX.png" alt="workflow" width="240" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2310.08864.pdf" id="workflow_arxiv">
                  <span class="papertitle"> Open X-Embodiment: Robotic Learning Datasets and RT-X Models</span>
                </a>
                <br>
                Open X-Embodiment Collaboration
                <br>
                <em>CoRL</em>, 2024
                <br>
                <a href="https://robotics-transformer-x.github.io/">project page</a> /
                <a href="https://arxiv.org/pdf/2310.08864.pdf">paper</a> /
                <a href="https://deepmind.google/discover/blog/scaling-up-learning-across-many-different-robot-types/">blog</a>
                <p>
                  This is an opensource dataset comprised of a large collection of robot embodiments. We study how vision-language 
                  models trained on X-Embodiment Datasets can enable efficient adaptation to new robots, tasks, and environments.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./static/images/RT2.png" alt="workflow" width="240" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://robotics-transformer2.github.io/assets/rt2.pdf" id="workflow_arxiv">
                  <span class="papertitle"> RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control</span>
                </a>
                <br>
                Google DeepMind Robotics
                <br>
                <em>ICRA</em>, 2023
                <br>
                <a href="https://robotics-transformer2.github.io/">project page</a> /
                <a href="https://robotics-transformer2.github.io/assets/rt2.pdf">paper</a> /
                <a href="https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/">blog</a>
                <p></p>
                <p>
                  We study how vision-language models trained on Internet-scale data can be incorporated directly into 
                  end-to-end robotic control to boost generalization and enable emergent semantic reasoning.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./static/images/ReDS.png" alt="workflow" width="240" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2211.01052.pdf" id="workflow_arxiv">
                  <span class="papertitle">Offline RL With Realistic Datasets: Heteroskedasticity and Support Constraints</span>
                </a>
                <br>
                <strong>Anikait Singh*</strong>, Aviral Kumar*, Quan Vuong, Yevgen Chebotar, Sergey Levine
                <br>
                <em>NeurIPS</em>, 2023
                <br>
                <a href="https://arxiv.org/pdf/2211.01052.pdf">paper</a> /
                <a href="https://recorder-v3.slideslive.com/#/share?share=88978&s=bedd4cb9-6463-416b-91bf-80d276f07bf3">talk</a>
                <p></p>
                <p>
                  CQL (ReDS) is an offline RL method that modifies a typical distribution constraint into an approximate 
                  support-level constraint via re-weighting to enable efficient learning from heteroskedastic dataset compositions.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./static/images/CALQL.jpeg" alt="workflow" width="240" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2204.05618.pdf" id="workflow_arxiv">
                  <span class="papertitle"> Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning</span>
                </a>
                <br>
                Mitsuhiko Nakamoto*, Yuexiang Zhai*, <strong>Anikait Singh</strong>, Max Sobol Mark, 
                <br>
                Yi Ma, Chelsea Finn, Aviral Kumar, Sergey Levine
                <br>
                <em>NeurIPS</em>, 2023
                <br>
                <a href="https://nakamotoo.github.io/Cal-QL/index.html">project page</a> /
                <a href="https://arxiv.org/pdf/2204.05618.pdf">paper</a> /
                <a href="https://youtu.be/r9CCdLeMJTg">video</a> /
                <a href="https://github.com/nakamotoo/Cal-QL">code</a>
                <p></p>
                <p>
                  A method that learns a conservative value function initialization that underestimates the value of the 
                  learned policy from offline data, while also being calibrated, in the sense that the learned Q-values 
                  are at a reasonable scale. This leads to effective online fine-tuning, enabling benefits of offline 
                  initializations in online fine-tuning
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./static/images/PTR.png" alt="workflow" width="240" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2210.05178.pdf" id="workflow_arxiv">
                  <span class="papertitle"> Pre-Training for Robots: Offline RL Enables Learning from a Handful of Trials</span>
                </a>
                <br>
                Aviral Kumar*, <strong>Anikait Singh</strong>*, Frederik Ebert*, Mitsuhiko Nakamoto
                <br>
                Yanlai Yang, Chelsea Finn, Sergey Levine
                <br>
                <em>RSS</em>, 2023
                <br>
                <a href="https://sites.google.com/view/ptr-final/">project page</a> /
                <a href="https://arxiv.org/pdf/2210.05178.pdf">paper</a> /
                <a href="https://www.youtubeeducation.com/watch?v=yAWgyLJD5lY">video</a>
                <p></p>
                <p>
                  PTR is a framework based on offline RL that attempts to effectively learn new tasks by combining 
                  pre-training on existing robotic datasets with rapid fine-tuning on a new task, with as few as 
                  10 demonstrations.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./static/images/offlinerlvsbc.png" alt="workflow" width="240" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2204.05618.pdf" id="workflow_arxiv">
                  <span class="papertitle"> When Should We Prefer Offline Reinforcement Learning Over Behavioral Cloning?</span>
                </a>
                <br>
                Aviral Kumar, Joey Hong, <strong>Anikait Singh</strong>, Sergey Levine
                <br>
                <em>ICLR</em>, 2022
                <br>
                <a href="https://bair.berkeley.edu/blog/2022/04/25/rl-or-bc/">project page</a> /
                <a href="https://arxiv.org/pdf/2204.05618.pdf">paper</a>
                <p></p>
                <p>
                  Theoretical paper that characterize the properties of environments that allow offline RL methods to 
                  perform better than BC methods, even when only provided with expert data. Additionally, policies trained 
                  on sufficiently noisy suboptimal data outperform BC algorithms with expert data, especially on long-horizon problems.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./static/images/workflow.png" alt="workflow" width="240" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2109.10813.pdf" id="workflow_arxiv">
                  <span class="papertitle">A Workflow for Offline Model-Free Robotic Reinforcement Learning</span>
                </a>
                <br>
                Aviral Kumar*, <strong>Anikait Singh*</strong>, Stephen Tian, Chelsea Finn, Sergey Levine
                <br>
                <em>CoRL</em>, 2021, <em>(Oral Presentation)</em>
                <br>
                <a href="https://sites.google.com/view/offline-rl-workflow">project page</a> /
                <a href="https://arxiv.org/pdf/2109.10813.pdf">paper</a> /
                <a href="https://www.youtube.com/watch?v=h9R5LJX9b1I&list=PL2oxSfYMr6hXwFk4_rBL1xgASdp-aOkV9&index=4">talk</a>
                <p></p>
                <p>
                  Our proposed workflow aims to detect overfitting and underfitting in model-free offline RL, 
                  and provides guidelines for addressing these issues via policy selection, regularization, and architecture design.
                </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./static/images/schepen.png" alt="workflow" width="240" style="border-style: none">
              </td>
              <td width="75%" valign="middle">
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8818288" id="workflow_arxiv">
                  <span class="papertitle"> A Mobile Application for Keyword Search in Real-World Scenes</span>
                </a>
                <br>
                Shrinivas Pundlik, <strong>Anikait Singh</strong>, Gautam Baghel, Vilte Baliutaviciute, Gang Luo
                <br>
                IEEE Journal of Translational Engineering in Health and Medicine, 2019
                <br>
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8818288">paper</a>
                <p></p>
                <p>
                  System to help visually-impaired patients localize where words are present in a cluttered environment. This system 
                  utilizes OCR + Levenshtein Distance along with specialized audio cues and additional assistive features to enable 
                  efficient and intuitive search in crowded, diverse environments.
                </p>
              </td>
            </tr>


          </tbody></table>

          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <hr>
            <tr>
              <td>
                <h2>Teaching</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./static/images/teaching.png" width="240" alt="csteaching">
              </td>
              <td width="75%" valign="center">
                Undergraduate Student Instructor, <a href="https://rail.eecs.berkeley.edu/deeprlcourse-fa22/">CS285 Fall 2022</a>
                <br>
                Undergraduate Student Instructor, <a href="https://inst.eecs.berkeley.edu/~cs188/sp22/">CS188 Spring 2022</a>
                <br>
                Undergraduate Student Instructor, <a href="https://rail.eecs.berkeley.edu/deeprlcourse-fa22/">CS285 Fall 2021</a>
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./static/images/stanford_logo.png" width="240" alt="csteaching">
              </td>
              <td width="75%" valign="center">
                Program Coordinator, Mentor, <a href="https://deeplearningportal.org/">Deep Learning Portal 2024</a>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <hr>
            <tr>
              <td width="100%" valign="center" halign="center">
                <p style="text-align:center">
                  Website template by <a href="https://jonbarron.info/">Jon Barron</a>.
                </p>
            </tr>
          </tbody></table>

        </td>
      </tr>
    </table>
    

  </body>

</html>
